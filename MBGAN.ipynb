{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MBGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNxd3M2jrX8wqGOJIJIxtvJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vlasovets/MB-GAN/blob/master/MBGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyJIzKFoCfpJ",
        "outputId": "46fc181b-4abc-4047-bccb-87c5d21e1586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MB-GAN'...\n",
            "remote: Enumerating objects: 173, done.\u001b[K\n",
            "remote: Counting objects: 100% (43/43), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 173 (delta 18), reused 8 (delta 0), pack-reused 130\u001b[K\n",
            "Receiving objects: 100% (173/173), 250.12 MiB | 33.19 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n",
            "Checking out files: 100% (66/66), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Vlasovets/MB-GAN.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd MB-GAN/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zbw6uzbfgKQ9",
        "outputId": "a20762b9-5525-4a78-9e71-0b38cada3f94"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MB-GAN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import os\n",
        "from model import *\n",
        "from utils import *\n",
        "from mbgan_train_demo import *\n",
        "from functools import partial\n",
        "\n",
        "from keras.layers import Input, Layer\n",
        "from keras.models import Sequential, Model"
      ],
      "metadata": {
        "id": "2WrmBLt8GVUI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator(input_shape, output_units, n_channels=512):\n",
        "    \"\"\" build the generator model. \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(n_channels, activation=\"relu\", input_shape=input_shape))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dense(n_channels))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dense(n_channels))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dense(output_units))\n",
        "    model.add(Activation(\"softmax\"))\n",
        "\n",
        "    noise = Input(shape=input_shape)\n",
        "    output = model(noise)\n",
        "\n",
        "    return Model(noise, output)"
      ],
      "metadata": {
        "id": "xRyFgm9sbqqX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_critic(input_shape, n_channels=256, dropout_rate=0.25, tf_matrix=None, t_pow=1000.):\n",
        "    \"\"\" build the critic model. \"\"\"\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(PhyloTransform(tf_matrix, input_shape=input_shape))\n",
        "    model.add(Lambda(lambda x: K.log(1 + x * t_pow)/K.log(1 + t_pow))) #EM-distance\n",
        "    model.add(Dense(n_channels))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(n_channels))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(n_channels))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    inputs = Input(shape=input_shape)\n",
        "    validity = model(inputs)\n",
        "    \n",
        "    return Model(inputs, validity)"
      ],
      "metadata": {
        "id": "NMOZJ18fbu4j"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomWeightedAverage(Layer):\n",
        "    \"\"\" Calculate a random weighted average between two tensors. \"\"\"\n",
        "    def _merge_function(self, inputs):\n",
        "        batch_size = K.shape(inputs[0])[0]\n",
        "        alpha = K.random_uniform((batch_size, 1, 1, 1))\n",
        "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])"
      ],
      "metadata": {
        "id": "dqJkRmKDdJs_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FILE = \"./data/raw_data.pkl\"\n",
        "\n",
        "data_o_case, data_o_ctrl, taxa_list = load_sample_pickle_data(FILE)\n",
        "\n",
        "adj_matrix, taxa_indices = expand_phylo(taxa_list)\n",
        "\n",
        "tf_matrix = adjmatrix_to_dense(adj_matrix, shape=(len(taxa_list), len(taxa_indices)))"
      ],
      "metadata": {
        "id": "LrxWgb6cGBiN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = {\n",
        "        'ntaxa': 719,\n",
        "        'latent_dim': 100,\n",
        "        'generator': {'n_channels': 512},\n",
        "        'critic': {'n_channels': 256, 'dropout_rate': 0.25, \n",
        "                   'tf_matrix': tf_matrix, 't_pow': 1000.}\n",
        "    }\n",
        "    \n",
        "train_config = {\n",
        "    'generator': {'optimizer': ('RMSprop', {}), 'lr': 0.00005},\n",
        "    'critic': {'loss_weights': [1, 1, 10], \n",
        "                'optimizer': ('RMSprop', {}), 'lr': 0.00005},\n",
        "}"
      ],
      "metadata": {
        "id": "juE9fC0JI58b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subsample from the real data"
      ],
      "metadata": {
        "id": "yfDZ2WBrb_fD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "real = data_o_case[np.random.randint(0, data_o_case.shape[0], 32)]\n",
        "real.shape\n",
        "# pd.DataFrame(real)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "qO6Qr9r6VvTd",
        "outputId": "191ab82a-36db-4f23-8e1e-ec72007cc3c0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e7733af4ac31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_o_case\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_o_case\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mreal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# pd.DataFrame(real)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data_o_case' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulate some noise signal"
      ],
      "metadata": {
        "id": "7-zCTo8ycDPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise = np.random.normal(0, 1, (32, model_config['latent_dim']))\n",
        "noise.shape"
      ],
      "metadata": {
        "id": "SCdk7MvOWtrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Determines fake sample from given noise"
      ],
      "metadata": {
        "id": "M-z2MTZCbQ4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = Input(shape=(model_config['latent_dim'],))\n",
        "z.shape"
      ],
      "metadata": {
        "id": "T6ZYlQ5ZXGJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = build_generator((model_config['latent_dim'],), model_config['ntaxa'])"
      ],
      "metadata": {
        "id": "inI2t38ccUFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "critic = build_critic((model_config['ntaxa'],))"
      ],
      "metadata": {
        "id": "G03kEnVLcs2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake_sample = generator(z)\n",
        "fake_sample.shape"
      ],
      "metadata": {
        "id": "aCaB3fsubWCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake = critic(fake_sample)"
      ],
      "metadata": {
        "id": "qUvSZD2UbddZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Determines real sample"
      ],
      "metadata": {
        "id": "29-QYqcFcoqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "real_sample = Input(shape=(model_config['ntaxa'],))\n",
        "valid = critic(real_sample)"
      ],
      "metadata": {
        "id": "Ig1UvZaHZ_Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Determines weighted average between real and fake sample\n",
        "        "
      ],
      "metadata": {
        "id": "qaw1BGZec5gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interpolated_sample = RandomWeightedAverage()([real_sample, fake_sample])\n",
        "validity_interpolated = critic(interpolated_sample)"
      ],
      "metadata": {
        "id": "6x9zverlc_O5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get gradient penalty loss"
      ],
      "metadata": {
        "id": "JMuPGwtedunC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "partial_gp_loss = partial(gradient_penalty_loss, averaged_samples=interpolated_sample)\n",
        "partial_gp_loss.__name__ = 'gradient_penalty'"
      ],
      "metadata": {
        "id": "r7VQWKowdwrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construct critic computational graph"
      ],
      "metadata": {
        "id": "bQNM5ZIneMmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "critic_graph = Model([real_sample, z])"
      ],
      "metadata": {
        "id": "-t9_sMmAW3LN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#standard keras optimizers adam, etc.\n",
        "optimizer = get_optimizer(train_config['critic']['optimizer'][0], lr=train_config['critic']['lr'])\n"
      ],
      "metadata": {
        "id": "9otmo1WBfRq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_weights = train_config['critic']['loss_weights']"
      ],
      "metadata": {
        "id": "8_6M8A5Jfz1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "critic_graph.compile(loss=[wasserstein_loss, wasserstein_loss, partial_gp_loss],\n",
        "            optimizer=optimizer, loss_weights=loss_weights,\n",
        "        )"
      ],
      "metadata": {
        "id": "Jkm-xba9f2bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_generator_graph(self):\n",
        "        \"\"\" Construct computational graph for generator. \"\"\"\n",
        "        # Freeze the critic's layers while training the generator\n",
        "        self.critic.trainable = False\n",
        "        self.generator.trainable = True\n",
        "        \n",
        "        # Generate sample and update generator\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        fake_sample = self.generator(z)\n",
        "        valid = self.critic(fake_sample)\n",
        "        \n",
        "        # Construct generator computational graph\n",
        "        self.generator_graph = Model(z, valid)\n",
        "        optimizer = get_optimizer(self.train_config['generator']['optimizer'][0], \n",
        "                                  lr=self.train_config['generator']['lr'], \n",
        "                                  **self.train_config['generator']['optimizer'][1])\n",
        "        self.generator_graph.compile(loss=wasserstein_loss, optimizer=optimizer)\n",
        "    "
      ],
      "metadata": {
        "id": "LuptUJ4DgN-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "critic.summary()"
      ],
      "metadata": {
        "id": "fSCsnMgkgnr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator.summary()"
      ],
      "metadata": {
        "id": "U3BQJq6Pgw4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GDNMWMY1hAoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NAME = \"mbgan_case\"\n",
        "EXP_DIR = \"NielsenHB_2014_stool\"\n",
        "\n",
        "mbgan = MBGAN(NAME, model_config, train_config)\n",
        "mbgan.__dict__"
      ],
      "metadata": {
        "id": "OGuqr9YOgWGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size=32\n",
        "# n_critic=5\n",
        "# n_generator=1 \n",
        "# save_interval=50\n",
        "# save_fn=None\n",
        "# experiment_dir=\"mbgan_train\"\n",
        "# verbose=0\n",
        "\n",
        "# valid = -np.ones((32, 1))\n",
        "# fake =  np.ones((32, 1))\n",
        "# dummy = np.zeros((32, 1))\n",
        "\n",
        "# for epoch in range(1, 5):\n",
        "#             for _ in range(n_critic):\n",
        "#                 # Randomly select a batch of samples to train the critic\n",
        "#                 real = data_o_case[np.random.randint(0, data_o_case.shape[0], 32)]\n",
        "#                 noise = np.random.normal(0, 1, (32, model_config['latent_dim']))\n",
        "#                 d_loss = critic_graph.train_on_batch([real, noise], [valid, fake, dummy])\n",
        "            \n",
        "#             # for _ in range(n_generator):\n",
        "#             #     #  Update the generator\n",
        "#             #     noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "#             #     g_loss = self.generator_graph.train_on_batch(noise, valid)\n",
        "\n",
        "#             # # Plot the progress\n",
        "#             # log_info = [\n",
        "#             #     \"iter={:d}\".format(epoch), \n",
        "#             #     \"[D loss={:.6f}, w_loss_real={:.6f}, w_loss_fake={:.6f}, gp_loss={:.6f}]\".format(*d_loss),\n",
        "#             #     \"[G loss={:.6f}]\".format(g_loss),\n",
        "#             # ]\n",
        "#             # print(\"{} {} {}\".format(*log_info))\n",
        "\n",
        "# generator_graph = Model(z, valid)\n",
        "\n",
        "# d_loss = critic_graph.train_on_batch([real, noise], [valid, fake, dummy])\n",
        "# critic_graph.train_on_batch([real, noise], [valid, fake, dummy])"
      ],
      "metadata": {
        "id": "JoAVMcKmjMLo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}